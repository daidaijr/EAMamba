train_dataset:
  dataset:
    name: denoising-color-dataset
    args:
      sigma_type: random
      sigma_range: [0, 50]
      gt_data_file: ./datasets/list/Denoise/DFWB.txt
      #! lq is generated by adding noise
      phase: train
      repeat: 1
      augment: True
  batch_size: 8   # Max batch size for progressive training
  crop_size: 384  # Max patch size for progressive training

valid_dataset:
  dataset:
    name: denoising-color-dataset
    args:
      sigma_type: constant
      sigma_range: 50
      gt_data_file: ./datasets/list/Denoise/McMaster.txt
      #! lq is generated by adding noise
      phase: valid
      repeat: 1
      augment: False 
  batch_size: 8
  crop_size: 128

model:
  name: eamamba
  args:
    inp_channels: 3
    out_channels: 3
    dim: 64
    num_blocks: [4, 6, 6, 7]
    num_refinement_blocks: 2
    ffn_expansion_factor: 2
    bias: False
    LayerNorm_type: 'WithBias'      # Other option 'BiasFree'
    dual_pixel_task: False          # True for dual-pixel defocus deblurring only. Also set inp_channels=6
    checkpoint_percentage: !!float 0.0    # ceil(percentage * num_blocks[i]) = # of segment for stage i
    channel_mixer_type: 'Simple'    # Change the channel mixer of every blocks, default to Simple, can be GDFN, FFN, CCA
    # SimpleGate require dim*ffn_expansion_factor to be even

    mamba_cfg:
    # ==== extras       ====
      scan_type: ~         # None(normal), diagonal, zorder, zigzag, hilbert
      scan_count: 4                 # 1, 2, 4, 8
      scan_merge_method: 'add'      # add, concate
      disable_z_branch: False
    # ==== mamba stats  ====
      d_state: 16                 
      d_conv: 3                     # SET to ODD NUMBER FOR CONV 2D
      expand: 1
      conv_2d: True                 # Conv1D if False, otherwise Conv2d 

e_decay: 0.999 # EMA model (model_e)

progressive_train:
  iters: [138000, 96000, 72000, 54000, 54000, 36000]     
  batch_sizes: [8, 5, 4, 2, 1, 1]             # Batch size per gpu                         
  crop_sizes: [128, 160, 192, 256, 320, 384]  # Patch sizes for progressive training

optimizer: 
  name: AdamW
  args:
    lr: !!float 3e-4           
    weight_decay: !!float 1e-4 
    betas: [0.9, 0.999]

lr_scheduler:
  name: CosineAnnealingRestartCyclicLR
  args:
    periods: [138000, 312000]
    restart_weights: [1, 1]
    eta_mins: [0.0003, 0.000001]  

total_iter: 450000
use_grad_clip: true

print_freq: 3000
val_freq: 3000
val_img_save_freq_mult: 10    # save val image every X val
save_checkpoint_freq: 30000   
save_last_freq: 300

resume: ~
